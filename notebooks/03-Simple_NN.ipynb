{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AI1uO_bdYDRH"
   },
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/guiwitz/MLCV/blob/main/notebooks/03-Simple_NN.ipynb)\n",
    "# 3. Neural networks with PyTorch\n",
    "\n",
    "Neural networks can be programmed on different levels depending on how much one needs to customize either the architecture or the training pattern. When dealing with more complex NN we will use a higher-level package (Lightning, see [Chapter 8](08-Lightning.ipynb)) which will spare us some \"manual\" work. However, in order to understand all steps involved in designing a DL algorithm, we will first use basic PyTroch. In this notebook we will see howe we can *design* a NN while in [the next notebook](05-Training.ipynb) we will se how to train it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aWCXSn5pOquE",
    "outputId": "5ac91b18-2672-43f3-d135-ae347bde8935",
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# set path containing data folder or use default for Colab (/gdrive/My Drive)\n",
    "local_folder = \"../\"\n",
    "import urllib.request\n",
    "urllib.request.urlretrieve('https://raw.githubusercontent.com/guiwitz/DLImaging/master/utils/check_colab.py', 'check_colab.py')\n",
    "from check_colab import set_datapath\n",
    "colab, datapath = set_datapath(local_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k5noZQBcOquF"
   },
   "source": [
    "## Layer from torch.nn\n",
    "\n",
    "We will see in this notebook how to create a simple neural network for image classification using the most general mechanism in PyTorch which are modules. All module-related objects and functions are contained in ```torch.nn```. As the name indicates, these modules are really *modular* in the sense that they are indpendent parts of networks that can be combined together. Mostly we will use single modules containing the multiple layers of our architecture, but we will also see examples where multiple modules are assembled.\n",
    "\n",
    "A module is essentially an assemble of multiple other modules, and many of these are provided \"pre-made\" for us also available in ```torch.nn```. For example in the simple network used here we will only use linear layers. Before we create an actual network let's see how layers work:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "YL9Tx6P0YDRM"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.manual_seed(42);\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m22IXJqLOquG"
   },
   "source": [
    "We creat now a linear layer that takes a vector of size 5 as input and ouptuts a vector of size 10:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "rMFmaxSuOquH"
   },
   "outputs": [],
   "source": [
    "lin_layer = nn.Linear(in_features=5, out_features=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qDQ4D0iuOquH"
   },
   "source": [
    "This single layer already has parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tEs08tFDOquI",
    "outputId": "fd94280b-776b-4549-95aa-ae7d31196d02"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[ 0.3419,  0.3712, -0.1048,  0.4108, -0.0980],\n",
       "         [ 0.0902, -0.2177,  0.2626,  0.3942, -0.3281],\n",
       "         [ 0.3887,  0.0837,  0.3304,  0.0606,  0.2156],\n",
       "         [-0.0631,  0.3448,  0.0661, -0.2088,  0.1140],\n",
       "         [-0.2060, -0.0524, -0.1816,  0.2967, -0.3530],\n",
       "         [-0.2062, -0.1263, -0.2689,  0.0422, -0.4417],\n",
       "         [ 0.4039, -0.3799,  0.3453,  0.0744, -0.1452],\n",
       "         [ 0.2764,  0.0697,  0.3613,  0.0489, -0.1410],\n",
       "         [ 0.1202, -0.1213,  0.1882,  0.3993,  0.2585],\n",
       "         [-0.1955,  0.2582,  0.0800,  0.2271, -0.2726]], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([-0.4427, -0.1728, -0.3430,  0.3670,  0.1288,  0.1852,  0.1414, -0.0078,\n",
       "          0.3500, -0.3178], requires_grad=True)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(lin_layer.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SIIb6gLAOquI"
   },
   "source": [
    "We see that we have indeed a 5x10 matrix plus the bias term of size 10. When we later assemble multiple modules, we will see the same time of output but with many more parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U-Kp9kzeYDRR"
   },
   "source": [
    "## Passing an input\n",
    "\n",
    "Our layer takes a vector as an input so let's create a tensor of size 5:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "BrpgmCV9YDRS"
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WgH4jbSpYDRS",
    "outputId": "c9536d97-c1a4-4a7b-bc59-54907e21b63f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([61, 23, 20, 11, 61])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mytensor = torch.randint(0,100,(5,))\n",
    "mytensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qXXbN8u2YDRS"
   },
   "source": [
    "No we we pass it to ```lin_layer```:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 307
    },
    "id": "lSXkG3RfYDRS",
    "outputId": "32af52b6-2fc8-47a8-db17-56a1f5d775c2",
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "expected scalar type Long but found Float",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mlin_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmytensor\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mambaforge/envs/CASImaging2023/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/mambaforge/envs/CASImaging2023/lib/python3.9/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: expected scalar type Long but found Float"
     ]
    }
   ],
   "source": [
    "lin_layer(mytensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.int64"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mytensor.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lin_layer.weight.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lV6W7QoJYDRT"
   },
   "source": [
    "The error can be a bit misleading. The problem here is that the weights in the layer are defined by default as ```float32```. Hence the input should match this but we passed an 64 bits integer which creates a conflict. Let's adjust the type of our tensor. We can either modify the tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "eCYVteuRYDRT"
   },
   "outputs": [],
   "source": [
    "mytensor_float = mytensor.float()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MlZQhZ8uOquW"
   },
   "source": [
    "Now we finally have the right object to path through our network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wwrmvWgcYDRU",
    "outputId": "e18c3b68-087a-4197-b1d5-f903e329a95d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 25.3977, -10.0996,  45.7222,  10.4237, -35.5498, -47.1532,  14.9067,\n",
       "         17.6140,  28.8162, -18.8349], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = lin_layer(mytensor_float)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-6bOnr9bYDRU",
    "outputId": "f4109c43-d388-4610-b1c8-17812d1f420e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lZXBQtGFOquX"
   },
   "source": [
    "We see that the ouput has as expected as size of 10! Of course we can now add as many other layers as we want:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "IvIVibm_OquX"
   },
   "outputs": [],
   "source": [
    "lin_layer2 = nn.Linear(10,3)\n",
    "output = lin_layer2(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vfCxymrWOquX",
    "outputId": "7b872674-8259-45fb-e8ef-aadc516128c1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gwgOrxZjOquY"
   },
   "source": [
    "## Activation\n",
    "\n",
    "In addition to *layers* we will typically also need activation functions such as soft-max. Those are implemented as layer objects in ```torch.nn``` as well as functions in ```torch.functional```:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "2-uqNcAGYDRN"
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gjZNAsaXOquY"
   },
   "source": [
    "Since we use the functional form, we can pass the output of the above linear layer directly to the activation function, here a ReLU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "GuIBiy7POquZ"
   },
   "outputs": [],
   "source": [
    "lin_layer_activated = F.relu(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZLiN8fnKOquZ",
    "outputId": "bae2c6f9-c1a1-4718-c989-2efc15af26dc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([37.3864,  0.0000, 34.4119], grad_fn=<ReluBackward0>)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lin_layer_activated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or we create a dedicated layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "relu_layer = nn.ReLU()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([37.3864,  0.0000, 34.4119], grad_fn=<ReluBackward0>)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relu_layer(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structure of a network\n",
    "\n",
    "There are two ways to assemble layers and activation functions into a NN architecture. In the first one, we just specify a sequence of layers and activations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "    nn.Linear(5,10),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(10,3)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TzPIWGlLYDRN",
    "tags": []
   },
   "source": [
    "This is very easy to implement for simple networks but limited to use more complex architectures such as Unets where one has to \"manually\" intervene e.g. to create links between layers. In that case we can implement the model as a calls of type ```nn.Module``` in which we can define layers, and combine them how we want in a ```forward``` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "-orgrI3IYDRO"
   },
   "outputs": [],
   "source": [
    "class Mynetwork(nn.Module):\n",
    "    def __init__(self, myparameter1, myparameter2):\n",
    "        super(Mynetwork, self).__init__()\n",
    "        \n",
    "        # define e.g. layers here e.g.\n",
    "        self.layer1 = nn.Linear(myparameter1, 5)\n",
    "        self.layer2 = nn.Linear(5, myparameter2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        # define the sequence of operations in the network including e.g. activations\n",
    "        x = F.relu(self.layer1(x))\n",
    "        x = self.layer2(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ij-rK7HkYDRO"
   },
   "source": [
    "Above, we have defined a simple network that is defined by two parameters, ```myparameter1, myparameter2```, and which is composed of two linear layers and ReLU unit. The differnt layers that we need are defined in ```__init__``` as object parameters and then re-used in the network definition in the ```forward``` function. ```forward``` takes an input ```x``` (e.g. an image to classify), passes it through the network and outputs the result. Therefore in principle we could instantiate a model and use it like this:\n",
    "\n",
    "```\n",
    "mymodel = Mynetwork(9,3)\n",
    "mymodel.forward(myinput)\n",
    "```\n",
    "\n",
    "However, ```nn.Module``` has a ```__call__``` attribute that allows us to use the class as a function like this:\n",
    "\n",
    "```\n",
    "mymodel = Mynetwork(9,3)\n",
    "mymodel(myinput)\n",
    "```\n",
    "\n",
    "This is actually **how a module should be properly used** in order to exploit all capabilities offered in PyTorch.\n",
    "\n",
    "Let's now try this out. We instantiate the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "4nhXAob2YDRO"
   },
   "outputs": [],
   "source": [
    "mymodel = Mynetwork(9,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tBL1nCf6YDRP"
   },
   "source": [
    "Just like for the single linear layer before, we can have a look at all parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HXGXjCKdYDRP",
    "outputId": "48b8e550-c441-4b9e-8a86-c88f14ec4dd3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[ 0.1865, -0.2321,  0.1675,  0.1513,  0.2381, -0.2557,  0.2397, -0.1576,\n",
       "           0.1237],\n",
       "         [ 0.3130, -0.0470, -0.0026, -0.0767, -0.2783,  0.1600, -0.3309,  0.2069,\n",
       "           0.2494],\n",
       "         [ 0.3152, -0.0786, -0.2739,  0.0749,  0.1841, -0.3318, -0.0757, -0.1998,\n",
       "          -0.0292],\n",
       "         [-0.1641, -0.1363, -0.1058, -0.3168,  0.2735,  0.2794, -0.0523, -0.0380,\n",
       "          -0.1360],\n",
       "         [-0.3010, -0.3244,  0.1239, -0.1830, -0.2143, -0.0260, -0.1110, -0.1078,\n",
       "           0.0107]], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([-0.0707, -0.1148, -0.1596, -0.2713,  0.2795], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[-0.1790,  0.1185, -0.1552,  0.0363,  0.4169],\n",
       "         [ 0.2060, -0.3876,  0.1775,  0.4245,  0.1177],\n",
       "         [ 0.2998,  0.4409, -0.0685,  0.0928, -0.3108]], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([-0.0922,  0.3312,  0.2293], requires_grad=True)]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(mymodel.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vht79uk8YDRQ"
   },
   "source": [
    "The output is the same, only now we see all parameters from all layers, not just a single one. We can also look at the modules contained in our network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sYxhkXAhOque",
    "outputId": "fc26bb8e-6a74-457d-cdb1-1d7f04f43108"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Mynetwork(\n",
       "   (layer1): Linear(in_features=9, out_features=5, bias=True)\n",
       "   (layer2): Linear(in_features=5, out_features=3, bias=True)\n",
       " ),\n",
       " Linear(in_features=9, out_features=5, bias=True),\n",
       " Linear(in_features=5, out_features=3, bias=True)]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(mymodel.modules())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j8NsX6vnOque"
   },
   "source": [
    "We see some repeats because we see here modules at all levels. E.g. each linear layer is a module but our entire network is a module as well.\n",
    "\n",
    "We can also just find all modules contained in our main module and recover its name and function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kp8wjdEPYDRQ",
    "outputId": "42b17056-73c6-4041-eda6-5fafdf4bdf87"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name: layer1 module: Linear(in_features=9, out_features=5, bias=True)\n",
      "name: layer2 module: Linear(in_features=5, out_features=3, bias=True)\n"
     ]
    }
   ],
   "source": [
    "for name, module in mymodel.named_children():\n",
    "    print(f'name: {name} module: {module}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lmhrocc9YDRR"
   },
   "source": [
    "We can also obtain a dictionary of all layers with their weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1MrDaJ4hYDRR",
    "outputId": "7f0c55d1-4438-4a25-9717-b15987c1f987"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('layer1.weight',\n",
       "              tensor([[ 0.1865, -0.2321,  0.1675,  0.1513,  0.2381, -0.2557,  0.2397, -0.1576,\n",
       "                        0.1237],\n",
       "                      [ 0.3130, -0.0470, -0.0026, -0.0767, -0.2783,  0.1600, -0.3309,  0.2069,\n",
       "                        0.2494],\n",
       "                      [ 0.3152, -0.0786, -0.2739,  0.0749,  0.1841, -0.3318, -0.0757, -0.1998,\n",
       "                       -0.0292],\n",
       "                      [-0.1641, -0.1363, -0.1058, -0.3168,  0.2735,  0.2794, -0.0523, -0.0380,\n",
       "                       -0.1360],\n",
       "                      [-0.3010, -0.3244,  0.1239, -0.1830, -0.2143, -0.0260, -0.1110, -0.1078,\n",
       "                        0.0107]])),\n",
       "             ('layer1.bias',\n",
       "              tensor([-0.0707, -0.1148, -0.1596, -0.2713,  0.2795])),\n",
       "             ('layer2.weight',\n",
       "              tensor([[-0.1790,  0.1185, -0.1552,  0.0363,  0.4169],\n",
       "                      [ 0.2060, -0.3876,  0.1775,  0.4245,  0.1177],\n",
       "                      [ 0.2998,  0.4409, -0.0685,  0.0928, -0.3108]])),\n",
       "             ('layer2.bias', tensor([-0.0922,  0.3312,  0.2293]))])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mymodel.state_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HAstg9EYOquf"
   },
   "source": [
    "Finally we can pass an input through our network. It takes an input of size 9:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "id": "BS7ZO74mOqug"
   },
   "outputs": [],
   "source": [
    "my_input = torch.randn((9,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "id": "NGVVZnV8Oqug"
   },
   "outputs": [],
   "source": [
    "output = mymodel(my_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0zE-tQpzOqug",
    "outputId": "0614a5d0-06b9-4c35-ad72-f0a0aecc9fa0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0596,  0.7805,  0.3574], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xKI4bj_YYDRU"
   },
   "source": [
    "## Batches\n",
    "\n",
    "If we want to use batch processing, we can also pass batches of vectors through the network instead of single vectors. PyTroch layers are designed to handle this by default, so here for example if we want to use a batch of size 32 we can just use a 32 x 8 tensor. We create one directly with ```torch.randn``` here (we will learn more about batches in the chapter on training):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b2FsgyCfYDRV",
    "outputId": "5458c8b3-b51c-4e72-8ed5-d5e3ab689294"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 9])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_tensor = torch.randn(32,9)\n",
    "batch_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "id": "bYAmmXLwYDRV"
   },
   "outputs": [],
   "source": [
    "batch_output = mymodel(batch_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UP48jBj2YDRV",
    "outputId": "e4ef09dc-4cfe-401f-85d8-851c6a89307b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 3])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VuIOlAVSOquj"
   },
   "source": [
    "We see that the output is \"batched\" as well, so the network gracefully handlend this batch for us."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dm35UQ76YDRV"
   },
   "source": [
    "## Passing an image as input\n",
    "\n",
    "One complexity when dealing with images, is making sure that the input/output dimensions (and we'll see later, dimensions inside the network) are appropriate. For example at the moment our network has a linear layer as input, so we would need to make sure our image is \"linear\".\n",
    "\n",
    "Let's consider an image from the MNIST dataset (images of handwritten numbers):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imageio.v3 as iio\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = iio.imread('https://datasets-server.huggingface.co/assets/mnist/--/mnist/train/0/image/image.jpg')\n",
    "image = torch.from_numpy(image)\n",
    "image = image.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(image);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to pass this image as input to a linear layer, we need to somehow concatenate all rows together. Obviously by doing that we loose interesting features in the image, and we'll see later how we can keep those using convolution later (note that this linearization is always present in the last layer of a classifier).\n",
    "\n",
    "To linearize this image, we can use the ```flatten``` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([784])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_flat = image.flatten()\n",
    "image_flat.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WJM4i32xOqul"
   },
   "source": [
    "Alternatively we can use the ```view``` method and instead of specifying axes, just use -1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tK7em6_XYDRW",
    "outputId": "253d5205-7c6c-42a7-c6ba-2835d0051f93"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([784])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_flat = image.view(-1)\n",
    "image_flat.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ugGq_DqGYDRW"
   },
   "source": [
    "To pass these $28*28 = 784$ values to our network we of course need to adjust the input size:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "id": "YKDedfAmYDRW"
   },
   "outputs": [],
   "source": [
    "mymodel = Mynetwork(784,20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "id": "2nHvFUbUYDRW"
   },
   "outputs": [],
   "source": [
    "output = mymodel(image_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B4I8aSoSYDRX",
    "outputId": "cc9327b3-0084-4ca7-a555-b96e9e710084"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-23.5123,  22.6542, -28.8260,  20.3478, -13.7348,  29.9258,   4.7831,\n",
       "        -30.3402,  27.6189, -17.3178, -27.2706, -15.8357, -44.8007,  26.7763,\n",
       "          6.5294, -10.0950,  39.2811,  17.6796,  37.0028, -12.5746],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TjTKDBKJYDRX"
   },
   "source": [
    "The operation of \"flattening\" can be done at different places. For example, instead of flattening the input, we can also flatten within our network definition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "id": "Xg_6FfmmYDRX"
   },
   "outputs": [],
   "source": [
    "class Mynetwork2(nn.Module):\n",
    "    def __init__(self, myparameter1, myparameter2):\n",
    "        super(Mynetwork2, self).__init__()\n",
    "        \n",
    "        # define e.g. layers here e.g.\n",
    "        self.layer1 = nn.Linear(myparameter1, myparameter2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = x.view(-1)\n",
    "        # define the sequence of operations in the network including e.g. activations\n",
    "        x = F.relu(self.layer1(x))\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "id": "FVwBTjLfYDRX"
   },
   "outputs": [],
   "source": [
    "second_model = Mynetwork2(784, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1de0QvgnYDRY",
    "outputId": "5d3122c7-5c38-4555-a4cc-31fb6a502146"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 32.3195,  22.5286,  64.9188,   0.0000,   0.0000,  72.3183,  19.4446,\n",
       "          0.0000,   4.9189,   0.0000,  11.2640, 101.6435,  22.4040,   0.0000,\n",
       "         45.0949,  25.2079,   0.0000,   0.0000,   3.9653,   0.0000],\n",
       "       grad_fn=<ReluBackward0>)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = second_model(image)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LB7DUByhYDRY",
    "outputId": "f075bcb8-701f-4955-9b47-ab128224ad95"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image batches\n",
    "\n",
    "We have to be a bit careful when operating with image batches. Let's start with manually importing two images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "image1 = iio.imread('https://datasets-server.huggingface.co/assets/mnist/--/mnist/train/0/image/image.jpg')\n",
    "image2 = iio.imread('https://datasets-server.huggingface.co/assets/mnist/--/mnist/train/1/image/image.jpg')\n",
    "image1 = torch.from_numpy(image1)\n",
    "image1 = image1.float()\n",
    "image2 = torch.from_numpy(image2)\n",
    "image2 = image2.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(1,2)\n",
    "ax[0].imshow(image1)\n",
    "ax[1].imshow(image2);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can assemble these two images into a stack:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_batch = torch.stack([image1, image2], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 28, 28])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_batch.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see from the dimensions that we indeed have now a batch of two images. The problem is that this will be interpreted by PyTorch as an image with multiple channels (like an RGB image) and not as a batch because the expected dimensions are ```Batch-Channel-PixelsX-PixelsY```. So we have to add an \"empty\" dimension between the batch dimension and the pixel dimensions using the ```unsqueeze``` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_batch = torch.unsqueeze(image_batch, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1, 28, 28])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_batch.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also now that we have a batch, the \"unwrapping\" of the image in a linear shape cannot be done simply using ```view(-1)``` or ```flatten```:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1568])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_batch.flatten().shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see above this concatenates all images in the batch into one large image. We have to specify that we want to leave the batch dimension *untouched*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 784])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_batch.flatten(start_dim=1).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a batch of two linearized images! If we update our model, we have now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mynetwork3(nn.Module):\n",
    "    def __init__(self, myparameter1, myparameter2):\n",
    "        super(Mynetwork3, self).__init__()\n",
    "        \n",
    "        # define e.g. layers here e.g.\n",
    "        self.layer1 = nn.Linear(myparameter1, myparameter2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = x.flatten(start_dim=1)\n",
    "        # define the sequence of operations in the network including e.g. activations\n",
    "        x = F.relu(self.layer1(x))\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can instantiate the model and pass our \"fake\" batch through it: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_batch = Mynetwork3(784, 3)\n",
    "out = model_batch(image_batch)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our output is indeed composed of a batch of 2, each element having 3 outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "87EQqckMYDRY"
   },
   "source": [
    "## Saving and loading a model\n",
    "\n",
    "Our next goal will be to train our network (see next chapter). At some point during or after training we will want to save both our model and its weights. This can be done in two ways.\n",
    "\n",
    "### Save full model\n",
    "\n",
    "First, we can save the entire model so that it can be simply reloaded. This is practical when developing but can lead to problems when moving saved models between computers. First we save our model (create a ````models```` folder in your directory if needed):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "OYkt0Fw0YDRY"
   },
   "outputs": [],
   "source": [
    "torch.save(second_model, datapath.joinpath('models/simpleNN.pt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oVR_snhCYDRZ"
   },
   "source": [
    "And reload it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "e4sjsS7KYDRZ"
   },
   "outputs": [],
   "source": [
    "third_model = torch.load(datapath.joinpath('models/simpleNN.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B5BcgpK9YDRZ",
    "outputId": "10bf12ae-5dd9-43c8-e0d4-a95c9ee21b68"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2.9879, 2.2543, 0.6235, 0.0000, 0.0000, 2.0040, 9.6056, 8.4799, 0.0000,\n",
       "        0.0000, 1.5722, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 5.2356,\n",
       "        0.0000, 0.0000], grad_fn=<ReluBackward0>)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "third_model(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hlCrSWAFYDRZ"
   },
   "source": [
    "And we see that we obtain the same values as above, meaning that the parameters were indeed saved.\n",
    "\n",
    "### Saving the model state\n",
    "\n",
    "Alternatively we can simply save all the parameters, which is a safer method. We simply recover them using ```state_dict```:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "OoBQM5drYDRZ"
   },
   "outputs": [],
   "source": [
    "torch.save(third_model.state_dict(),datapath.joinpath('models/simpleNN_state.pt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WORns8K-YDRZ"
   },
   "source": [
    "To reload the parameters we of course now have to first instantiate the model and \"fill\" it with those values using ```load_state_dict```:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "wGZ_UAqhYDRZ"
   },
   "outputs": [],
   "source": [
    "fourth_model = Mynetwork2(784, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WxQoeTofYDRa",
    "outputId": "0738c446-3054-4660-878e-9d83f4a25e24"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('layer1.weight',\n",
       "              tensor([[-0.0037,  0.0130,  0.0145,  ...,  0.0164, -0.0147,  0.0120],\n",
       "                      [ 0.0044,  0.0146, -0.0086,  ..., -0.0005,  0.0144, -0.0196],\n",
       "                      [ 0.0279, -0.0238, -0.0311,  ..., -0.0305,  0.0024,  0.0009],\n",
       "                      ...,\n",
       "                      [ 0.0090, -0.0278, -0.0105,  ...,  0.0123,  0.0218,  0.0213],\n",
       "                      [ 0.0242,  0.0156,  0.0108,  ...,  0.0154, -0.0153,  0.0217],\n",
       "                      [-0.0239,  0.0014,  0.0069,  ..., -0.0070, -0.0050, -0.0247]])),\n",
       "             ('layer1.bias',\n",
       "              tensor([-0.0300,  0.0135,  0.0126, -0.0301,  0.0079, -0.0219,  0.0166, -0.0311,\n",
       "                      -0.0126,  0.0305, -0.0181, -0.0226,  0.0038, -0.0043, -0.0073,  0.0127,\n",
       "                       0.0165,  0.0203, -0.0097,  0.0190]))])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = torch.load(datapath.joinpath('models/simpleNN_state.pt'))\n",
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OEBX9c-nYDRa",
    "outputId": "3d02cff2-bde0-4b77-c11a-d0ef24155f74"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fourth_model.load_state_dict(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "z9oYFeQhYDRa",
    "outputId": "c1b147dd-558d-4666-d27a-752c46fff353"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2.9879, 2.2543, 0.6235, 0.0000, 0.0000, 2.0040, 9.6056, 8.4799, 0.0000,\n",
       "        0.0000, 1.5722, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 5.2356,\n",
       "        0.0000, 0.0000], grad_fn=<ReluBackward0>)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fourth_model(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4x3iMsO-YDRa"
   },
   "source": [
    "## Running on a GPU (run on Colab)\n",
    "\n",
    "If you want to exploit the computational power of a GPU you have to take care of moving **both** the model and the data to it. First, find out if you have a GPU. You can test this for example by running this notebook on Colab:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GmDZf0pBYDRa",
    "outputId": "9a7c301a-9dd7-4aa4-fb4e-b8d349b10e5b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rxxUBfwOYDRb"
   },
   "source": [
    "If you have a GPU, you can define it as a device. If not you can fall back on CPU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "id": "A-L5ErcoYDRb"
   },
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    dev = torch.device(\"cuda\")\n",
    "else:\n",
    "    dev = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s_arWP1_YDRb"
   },
   "source": [
    "Now finally if you want to use it with a model you need to move the model to it. Let's check where it currently sits:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "V0wwfn2lY2am",
    "outputId": "f28d0b50-b1f0-4e25-a745-55039fd14105"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Mynetwork2(\n",
       "  (layer1): Linear(in_features=1024, out_features=20, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "second_model.to(dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vA7pOTxXawjR",
    "outputId": "b8e2e6f9-5b4e-4071-b8ae-451535e62c68"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[-6.5818e-03, -1.3050e-05, -2.1981e-02,  ...,  2.0436e-02,\n",
       "          -1.8892e-03, -2.1725e-02],\n",
       "         [ 2.0734e-02, -1.9473e-02, -2.8260e-02,  ...,  2.5199e-02,\n",
       "          -2.9951e-02,  8.9792e-03],\n",
       "         [-1.3109e-03,  1.3450e-02,  1.9928e-02,  ..., -5.7713e-03,\n",
       "           1.0590e-02, -1.9134e-02],\n",
       "         ...,\n",
       "         [ 1.6805e-02,  2.3167e-02,  3.1948e-03,  ..., -1.5454e-02,\n",
       "           1.3553e-02, -1.7430e-02],\n",
       "         [-2.3587e-02, -1.8857e-02,  9.4816e-03,  ...,  2.1235e-02,\n",
       "          -2.0854e-02, -2.8723e-02],\n",
       "         [ 2.1510e-03, -4.4486e-03, -1.7398e-02,  ..., -1.2099e-02,\n",
       "           1.0446e-02,  2.7855e-03]], device='cuda:0', requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([ 0.0084,  0.0012,  0.0020,  0.0185,  0.0250, -0.0038,  0.0236, -0.0299,\n",
       "          0.0036,  0.0277, -0.0210, -0.0126, -0.0142, -0.0256, -0.0099,  0.0236,\n",
       "          0.0120, -0.0239,  0.0216, -0.0094], device='cuda:0',\n",
       "        requires_grad=True)]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(second_model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N-_sgXVsbPOU"
   },
   "source": [
    "Now if we try to pass our current image through the network we get an error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 324
    },
    "id": "FAqQgKribVaX",
    "outputId": "107961eb-c1a1-49a6-c6f7-a999ec035318"
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-62-3dd5587f259c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msecond_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-44-4a29866e6538>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;31m# define the sequence of operations in the network including e.g. activations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1846\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhas_torch_function_variadic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1847\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1848\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1850\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument mat2 in method wrapper_mm)"
     ]
    }
   ],
   "source": [
    "second_model(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fM379mcZYDRc"
   },
   "source": [
    "The error message is quite explicit: you then need also to move the data, both for training and inference to the GPU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "prB7rQBoYDRc",
    "outputId": "1e3c9bf7-54f3-4378-8551-b3bbb982bba9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image.to(dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "id": "sGgYqbWpYDRc"
   },
   "outputs": [],
   "source": [
    "image = image.to(dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "id": "_50Ib-_xYDRc"
   },
   "outputs": [],
   "source": [
    "output = second_model(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dDX-VKv9bqrj",
    "outputId": "4324a258-07a2-4eaf-9830-626b43738410"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0000,  0.0000,  0.0000,  0.0000,  4.2747, 12.7591,  0.0000,  0.0000,\n",
       "         3.2894,  3.1473,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "        14.3413,  7.2331,  0.0000,  0.0000], device='cuda:0',\n",
       "       grad_fn=<ReluBackward0>)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Oor9GgLbcKXX"
   },
   "source": [
    "The output sits on the GPU and is part of the gradient calcualtion, so if you want to further use it, you will have to pull it from the GPU **and** detach it from gradient calculation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pY-wZED3cNBd",
    "outputId": "68e430e0-b16f-4b1f-f2ff-0358eef59503"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.       ,  0.       ,  0.       ,  0.       ,  4.2747416,\n",
       "       12.759116 ,  0.       ,  0.       ,  3.2893572,  3.1472843,\n",
       "        0.       ,  0.       ,  0.       ,  0.       ,  0.       ,\n",
       "        0.       , 14.341337 ,  7.2330832,  0.       ,  0.       ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fgj-fk6pOqux"
   },
   "source": [
    "## Exercises\n",
    "\n",
    "1. Create a network with 4 successive linear layers of size 20,40 and 100 and 2, and ReLU activation after the three first layers. It takes a 2D input.\n",
    "2. With ```imageio``` import the image in ```.../data/woody_baille.JPG```\n",
    "3. Instantiate your model with the appropriate size\n",
    "4. Run the image through the network and turn the output into a numpy array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ioXlvZY3Oqux"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "hlCrSWAFYDRZ"
   ],
   "name": "Simple_NN.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
